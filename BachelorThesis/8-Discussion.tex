\newpage
\thispagestyle{empty}
\mbox{}

\chapter{Discussion}
\section{Introduction}
In this chapter, the group explains why they made certain choices during the pipeline construction, discusses any changes they made to the project, and identifies any limitations they faced.

\section{The groups use of the SDLC} %Bytte navn perhaps? 
During the thesis project, the group developed a report and code. While it is not technically software, the process shares similarities with software development, as a result, they decided to explore the use of the \acrshort{sdlc} in their work.
%\\~\\
During the planning phase, the group focused on gathering requirements for the thesis and developing a project plan. Initially, the group created a plan for using third-party tools for the scans and set an estimate of the costs. However, the plan of using third-party tools was later discarded, except for the \acrshort{dast} tool. The project plan included a Gantt chart outlining the expected task timeframe. Additionally, the group utilized Jira\footnote{Available at \url{https://www.atlassian.com/software/jira}} to create a Kanban board for task assignments. The group created several models to prepare for pipeline implementation that detailed its structure, including security scans. The group also became familiar with the software and tools that would be utilized for the pipeline.
%\\~\\
In the implementation phase, the group began the development of the pipeline while simultaneously working on the thesis - the coding process for the pipeline involved repeatedly implementing code and testing its functionality. For instance, the group implemented the code for \acrshort{aws} CodeBuild and tested whether the service was created in accordance with the code. As a result, the implementation and testing phases were carried out iteratively.
%\\~\\
The result was ultimately merged into a GitHub repository and documented in the thesis. However, as the product is not an application and is not intended for direct use in the future but rather as a demonstration, plans have yet to be made for its maintenance. 

\section{Chosen branch protection rules}
When deciding which branch protection rules to implement, it is essential that securing the branch is relatively manageable for the workflow. Several possible protection rules to enable are described in section \ref{branchprotection}. Though enabling all rules at once may do more harm than good, every pull request and commit must go through several steps, making the workflow less efficient. Therefore, selecting only the most valuable rules will benefit the development. 
\\~\\
Enabling "Require a pull request before merging" is based on the "Four Eyes Principle," which is described more in detail in section \ref{Security of the pipeline} \cite{foureyes}. By enabling this rule, the risk of unwanted code being merged into the basecode is lower, without disrupting the work of all developers since only two need to look through the code. 
\\~\\
Another implemented branch rule requires signed commits. Signed commits require some pre-configurations, but once that is done, all the developers must enter a passphrase, and the commit is verified. Implementing required signed commits ensures the integrity of the code while the workflow is undisturbed.

\section{The different tools chosen}
After evaluating multiple tools for securing the pipeline, the group has given perspectives and recommendations regarding the various scans. Although the group decided to use integrated tools within GitHub for performing \acrshort{sast} and \acrshort{sca} testing, as well as a third-party tool for \acrshort{dast} testing, the decision was primarily based on their suitability for the given task and Erik Hjelmås' recommendation (see the meeting in Appendix \ref{møteErik}). It is crucial to note that this does not necessarily imply that others should adopt the same tools. While conducting such tests is crucial in application development, selecting tools for carrying out these tests should be based on specific requirements. Therefore, exploring the various tools available for different tests and assessing whether they meet one's specific needs is essential. The tools selected by the group are examples of what can be used. 

\subsection{Why OWASP ZAP was chosen}
In the course of researching security tools, an evaluation was made of the three tool types: \acrshort{sast},  \acrshort{dast}, and \acrshort{sca}. As neither GitHub nor \acrshort{aws} provides a DAST tool, a third-party tool had to be used. Among these, the \acrshort{owasp} \acrshort{zap} appeared as the most widely used web application scanner globally. This tool is convenient due to its user-friendly interface and straightforward setup process, suitable for individuals of all skill levels. Furthermore, it is open-source and available to all users without additional costs. Its integration into \acrshort{aws} pipelines was found to be a simple and uncomplicated process.

\subsection{Why tools integrated into GitHub was chosen}
Initially, the thesis aimed to investigate third-party tools, like Snyk and Mend, which could be integrated into GitHub and run scans on the code pushed to the repository. Then, however, it was decided to look into the in-built tools within GitHub and \acrshort{aws} that perform \acrshort{sast}, \acrshort{dast}, and \acrshort{sca} tests. The goal was to explore the potential benefits of using pre-integrated tools to create a secure pipeline and give priority to overall functionality rather than choosing individual tools.

\subsection{The group's experience with CodeQL}
The group used CodeQL as their \acrshort{sast} tool, which, as mentioned above, is an integrated tool in GitHub and is set as default when enabling Code Scanning. As a result, the group found it uncomplicated to set up and customize it for personal use. For example, the tool was configured to be triggered only when a push to the main branch was done, which was easily achievable. 
\\~\\
The group created a separate repository where OWASP Juice-Shop code was forked and then enabled CodeQL for this repository. After enabling CodeQL, the group enabled notification, so every time it found a vulnerability, all the group members would be notified by email. However, due to the many vulnerabilities within the code, the group quickly decided not to continue with the notifications since the group got a significant amount of emails. Nonetheless, if the group decided only to be notified every time the tool found vulnerabilities considered high or critical, the number of emails would have been significantly lower. Even though the group decided to turn off the notifications, it continued to find vulnerabilities. Lastly, CodeQL, in collaboration with Dependabot, managed to find all 101 vulnerabilities\footnote{Available at: https://owasp.org/www-project-juice-shop/} reported in the vulnerable-by-design web application OWASP Juice Shop. 
\\~\\
However, JuiceShop is a widely used repository, which might be why CodeQL and Dependabot managed to find all the vulnerabilities. To conduct a more comprehensive evaluation of the tool, the group could test it on additional repositories that contain vulnerabilities. They could also test it on personal code with vulnerabilities known only to the group. That would have made the result even more accurate, giving CodeQL various repositories with different vulnerabilities.
\\~\\
However, the group found the tool quite intuitive and thought it was practical that when it found an error, it explained thoroughly what the warning was and even gave an example of how to patch it. The group did not look into the example and saw if it could be considered a good patch, but thought it was good to at least receive a possible fix. It also refers to CWE vulnerabilities. 
\\~\\
In summary, the group found the tool practical and straightforward to install. However, they only tested it with JuiceShop, so it is difficult to determine its accuracy with other repositories. The group did not investigate further as this was beyond the scope of their project.

\subsection{The group's experience with OWASP ZAP}

The group opted for OWASP ZAP as \acrshort{dast} tool and found it relatively easy to set up. However, rather than customizing their scans, the group utilized pre-made functions due to limited documentation on customizing their own scans and time constraints. Creating scans would quickly be too complicated and therefore take too much time. 
\\~\\
When OWASP ZAP was finally up and running with basic configurations, the group encountered difficulties understanding the results' output and extracting the output from the docker container. 
\\~\\
In contrast to the SAST tool, the group also used JuiceShop code for the  DAST scan, which was quite effective. 
%I will add more here once we have looked at the dast scan. 



\subsection{The group's experience with Dependabot}
Dependabot was used as \acrshort{sca} tool and was relatively easy to set up. When compared to both \acrshort{dast} and \acrshort{sast}, the group ran JuiceShop code through Dependabot, which, as mentioned above, managed together with \acrshort{sast} to find all the vulnerabilities within the code. 
Compared with CodeQL, Dependabot provides a detailed explanation of the error and suggests recommendations for fixing it. For instance, if an outdated dependency version has been detected, Dependabot recommends which version to update. If there is yet to be an update, Dependabot suggests another dependency should be used. However, it does not come with any alternative dependencies. The group found this helpful feature, as it provides suggestions for maintaining security. However, it is important to note that the group did not investigate the suggested fixes, as it was beyond the project's scope.
\\~\\
In summary, the group found Dependabot valuable due to its ease of setup. However, the group also identified some areas where it could be improved, such as the lack of examples for alternative dependencies that could be used in case a vulnerability is detected in the current dependency. 

\subsection{The group's experience with Secret Scanning}
Finally, the group explored Secret Scanning, which, as mentioned, is also an integrated tool in GitHub. Although Secret Scanning is not necessarily an application security testing, the group found it valuable as it scans the code for secrets such as tokens and \acrshort{api} keys, which could allow access to sensitive information. If such secrets are exposed, they can be exploited by an attacker to steal data, for instance. 
\\~\\
Since \acrshort{sast} tools can be used to scan the code for vulnerabilities that an attacker can exploit, the group considered it good practice to use another scanner that also checks for secrets. 
\\~\\
Secret Scanning was also relatively easy to set up compared to different application security tests. Thus, the group decided it would be beneficial to use it. It enhances security and can also be configured to trigger when specific events occur, making it a good tool to use. 
\\~\\
Secret Scanning also gives a detailed explanation of the errors and how to solve them, which is what GitHub tools usually do. 

\section{Automation}
Automation is using \gls{iac} to perform tasks instead of doing them manually \cite{automationredhat}. Implementing automation in the organization's systems can eliminate the need for many developers to manage all the different infrastructure elements the organization may have. Furthermore, there are numerous advantages to replacing manual work with automation. It can reduce costs by replacing the manual work of IT professionals with automated processes or by reducing the need for physical hardware, such as servers, by moving to a cloud-based solution. Automation can also speed up development by automating repetitive tasks like testing, building, and deploying code, allowing developers to work on more complex jobs \cite{automation}. In addition, it can improve security by reducing the risk of human errors or by automating security testing that can help speed up the detection, verification, and escalation of security issues without requiring manual involvement. The group's decision to use Terraform and construct a pipeline with \gls{iac} is mostly based on the abovementioned benefits. Furthermore, the group attempted to automate as many processes as possible to remove the need for manual involvement during deployment and testing. 
\\~\\
Despite the numerous benefits of automating the development processes, the implementation can be time-consuming and require a complex setup. However, once completed, the setup would be significantly less complicated and require less time. One significant benefit of automating the development process is the ability to reuse the automated configuration across several projects \cite{reusepipeline}. Automating the process once may be reused in various settings, allowing numerous projects to benefit from the same improved pipeline. 
\\~\\
A goal should be to create an \gls{idempotent} process that assures the pipeline's output is consistent every time it is built \cite{idempotent}. Adopting idempotence as a practice in DevOps is an approach during application development that ensures a high-quality experience for both users and software teams. Idempotence eliminates the requirement for post-deployment cleanup, lowering the likelihood of errors. 

\section{The Use of Framework}
\subsection{SLSA}
Consistency is crucial when building the code, as mentioned in \acrshort{slsa} level 1. Therefore, it is mandatory to have a method of instructing others on building the code so that the outcome is consistent every time. The most common way of achieving this, and the method our group has chosen, is through a README.md file in the code repository. 
\\~\\
The second point the group had to deal with was to have a build platform that could automatically create \gls{provenance}, creating a document that provides information about what was built, the method used to build it, and the exact time it was built. This is where the problem with the \acrshort{slsa} framework and \acrshort{iac} code arises. Many programming languages have a requirement to build them before running them. However, in the case of \acrshort{iac} languages, it is unnecessary to build them beforehand.
\\~\\
Failing to achieve point two makes it challenging to proceed to point three in \acrshort{slsa} level 1. Without generating a provision, we have no content to share publicly with other users. It is worth noting that this obstacle also applies to levels 2 and 3, as completing level 1 is a prerequisite for advancing to these levels.

\subsection{SSDF}
While numerous models exist for the \acrlong{sdlc}, only a few prioritize security. As a result, it is necessary to integrate security into the \acrshort{sdlc} models. Utilizing a security-focused framework, such as the \acrshort{ssdf}, as a reference can serve as a starting point for enhancing the security of the \acrshort{sdlc}. 
\\~\\
In \acrshort{ssdf} documentation, elaborated in section \ref{ssdf}, includes various practices covering various security aspects that can be implemented into the development process. While the SSDF is a suitable framework for secure software development, the group did not explicitly consider it while developing and designing the pipeline in the thesis. However, the group believes that the practical work in the thesis aligns with many of the practices and principles of the SSDF, even though it was not explicitly followed. The group reviewed some examples of the practices they believed to be suitable and their implementation methods. The group has decided to review the practices category-wise, with each practice being assigned to a specific category. 
\\~\\
\textbf{Under Prepare the Organization (PO):}
Implement Supporting Toolchains (PO.3): \textit{\say{Use automation to reduce human effort and improve the accuracy, reproducibility, usability, and comprehensiveness of security practices throughout the SDLC, as well as provide a way to document and demonstrate the use of these practices. Toolchains and tools may be used at different levels of the organization, such as organization-wide or project-specific, and may address a particular part of the SDLC, like a build pipeline.}}\cite{ssdf}
\\~\\
This practice recommends selecting the right tools for the toolchain, following the recommended security practices, and utilizing tools to generate artifacts. In order to accomplish this, the group used various security tools to run different security tests, all of which comply with best practices for supply chain security. One of the key security practices the group has implemented is to use code-based configuration by automating the pipeline with Terraform. The automation of the pipeline not only ensures that the pipeline is efficient and practical but also promotes consistency and reduces the likelihood of errors. In \acrshort{aws}, the implementation of this practice is shown by the fact that each stage of the pipeline generates an artifact stored in an S3 bucket. This ensures the artifacts created are safe, secure, and available for future use.
\\~\\        
\textbf{Protect Software (PS):}
Protect All Forms of Code from Unauthorized
Access and Tampering (PS.1): \textit{\say{Help prevent unauthorized changes to code, both inadvertent and intentional, which could circumvent or negate the intended security characteristics of the software. For code that is not intended to be publicly accessible, this helps prevent theft of the software and may make it more difficult or time-consuming for attackers to find vulnerabilities in the software.}}\cite{ssdf}
\\~\\
This practice focuses on maintaining the integrity and availability of code, as well as proper storage of all code forms. To keep track of the modifications to the code, the group utilizes GitHub's version control and the artifacts generated in \acrshort{aws} if necessary. The group also uses signed commits to ensure code integrity. To follow the \say{least privilege} principle, the group decided to give each member access based on their assigned tasks. Therefore, the code repository was accessible in read-only to the two members responsible for writing the report. In comparison, the other two members, responsible for practical work, were granted full access. 
\\~\\
\textbf{Produce Well-Secured Software (PW):} Review and/or Analyze Human-Readable
Code to Identify Vulnerabilities and Verify Compliance with Security Requirements (PW.7): \textit{\say{Help identify vulnerabilities so that they can be corrected before the software is released to prevent exploitation. Using automated methods lowers the effort and resources needed to detect vulnerabilities. Human-readable code includes source code, scripts, and any other form of code that an organization deems human-readable.}}\cite{ssdf}
\\~\\
This practice covers code security and provides guidelines for evaluating the source code used or written. The evaluation can be done through manual code review or automated tools. The goal is to identify and correct vulnerabilities before releasing the software to prevent exploitation. The group has incorporated various automated security scans throughout the pipeline, such as SCA, SAST, and DAST. 
\\~\\
\textbf{Respond to Vulnerabilities (RV):}
Identify and Confirm Vulnerabilities on an Ongoing Basis (RV.1): \textit{\say{Help ensure that vulnerabilities are identified more quickly so that they can be remediated more quickly in accordance with risk, reducing the window of opportunity for attackers.}}\cite{ssdf}
\\~\\
This practice includes doing frequent vulnerability scans, threat assessments, and penetration testing, as well as monitoring and analyzing system logs and network traffic for indications of possible security problems. Various automated security scans, such as \acrshort{sca}, \acrshort{sast}, and \acrshort{dast} tests, have been implemented throughout the pipeline. These scans help detect potential vulnerabilities and security concerns in the code. Apart from the scans, the group also reflected on the option of application monitoring. This involves conducting assessments post-deployment to ensure the application remains secure and no new vulnerabilities have emerged.

\subsection{Usefulness of the framework}

Both the SLSA and SSDF frameworks may give helpful recommendations to developers trying to improve the security of their products. However, while SLSA has the potential to encourage good code heritage and increase security in more significant open-source projects with larger user bases, its complexity may not be justified in smaller projects. SSDF, on the other hand, provides a flexible way to apply security measures, making it a helpful guide for deciding which measures to deploy. Furthermore, the SSDF may be used by developers to assess the security level of their project without needing an extra step in the development process. Ultimately, the project's unique demands and scope will determine the decision between the two frameworks.

\section{Revising the thesis angle}
When writing the project plan, the group initially planned to write the thesis where various security testing tools would be tested and analyzed and later demonstrate how the different tools could be used together. The main focus was on the tools and testing as many tools as possible. The group found it challenging to create a unique thesis due to previous theses with similar topics. 
\\
After a meeting with the group's professor Erik Hjelmås  (\ref{møteErik}), discussing this issue, the group found an alternative approach for the thesis based on the discussion with Erik and a report from Usenix \cite{usenixreport} shown to the group. The new approach involved incorporating more practical work with \gls{iac} and focusing on utilizing tools already integrated into GitHub and \acrshort{aws}, and implementing best practice security measures. This approach of the thesis seemed to be more aimed at professional life and what kind of research was needed today.  


\section{Expectations compared to reality}
\subsection{Practical work}
As mentioned in the section \ref{section: Knowledge that had to be acquired}, the group needed to learn more about \gls{iac}, especially not Terraform. During this process, the group faced more issues than anticipated while building the pipeline with Terraform and \acrshort{aws}. The group had some experience with Microsoft Azure from previous courses and perhaps thought that \acrshort{aws} was more similar to Azure than it is. Additionally, \acrshort{aws} provides a wide range of services that targets the same or similar issues. The process of selecting these services and integrating them has posed a more significant challenge than initially anticipated. 

\subsection{Research}
The group struggled to find proper academic research for their thesis. Therefore, the majority of their resources came from websites or blog posts. To establish the credibility of these sources, the group either confirmed the author's or company's credibility or cross-referenced the information with additional sources to confirm its accuracy. These precautions were taken to assure the accuracy and validity of the data in the thesis.


\section{Critique of the thesis}

\subsection{Not using framework from the beginning}
The group did not look deeper into following a framework as the scope contained no specifications. However, when the group discovered different frameworks, the group decided that it was too late to integrate them into the work. Doing so would have required a significant amount of work, and since the group was too far into the thesis, it was decided not to look into it—many requirements for the different frameworks needed to be achieved before the thesis was started. Furthermore, many tasks must be completed if the group were to integrate the framework later in the thesis, which would have added more complexity. 

\subsection{Defining the scope}
Initially, the group needed help understanding the stakeholder's scope and the following requirements. The group was given many leeways and was told to interpret the scope however suited, which made it difficult to establish what the scope was asking due to a lack of knowledge about the topic. As a result, the group had to invest more time and work in the beginning to ensure that the thesis became the best possible. As mentioned above, the group, for example, talked with Erik Hjelmås regarding the scope and how to optimize it. 

