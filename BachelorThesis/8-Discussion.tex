\newpage
\thispagestyle{empty}
\mbox{}

\chapter{Discussion}
\section{Introduction}
This chapter details the reasons behind the group's decision-making process during the pipeline construction. It also covers any modifications made to the project and highlights any limitations encountered.

\section{Implementation of the SDLC}
Throughout the thesis, the group created a report and code, and while doing so, they chose to explore the implementation of the \acrshort{sdlc} approach.
\\~\\
During the planning phase, the group focused on gathering requirements for the thesis and developing a project plan. Initially, the group created a plan for using third-party tools for the scans and set an estimate of the costs. However, the plan of using third-party tools was later discarded, except for the \acrshort{dast} tool. The project plan included a Gantt chart outlining the expected task timeframe. Additionally, the group utilized Jira\footnote{Available at \url{https://www.atlassian.com/software/jira}} to create a Kanban board for task assignments. The group created several models to prepare for pipeline implementation that detailed its structure, including security scans. The group also became familiar with the software and tools that would be utilized for the pipeline.
\\~\\
In the implementation phase, the group began the development of the pipeline while simultaneously working on the thesis. The coding process for the pipeline involved repeatedly implementing code and testing its functionality. For instance, the group implemented the code for \acrshort{aws} CodeBuild and tested whether the service was created in accordance with the code. As a result, the implementation and testing phases were carried out iteratively.
\\~\\
The result was ultimately merged into a GitHub repository and documented in the thesis. However, as the product is not an application and is not intended for direct use in the future but rather as a demonstration, plans have yet to be made for its maintenance. 

\section{Chosen branch protection rules}
When deciding which branch protection rules to implement, it is essential that securing the branch is relatively manageable for the workflow. Several possible protection rules to enable are described in section \ref{branchprotection}. Enabling all rules at once may do more harm than good, every pull request and commit must go through several steps, making the workflow less efficient. Therefore, selecting only the most valuable rules will benefit the development. 
\\~\\
Enabling \say{Require a pull request before merging} is based on the \say{Four Eyes Principle}, which is described more in detail in section \ref{Security of the pipeline} \cite{foureyes}. By enabling this rule, the risk of unwanted code being merged into the basecode is lower, without disrupting the work of all developers since only two need to look through the code. 
\\~\\
Another implemented branch rule requires signed commits. Signed commits require some pre-configurations, but once that is done, all the developers must enter a passphrase, and the commit is verified. Implementing required signed commits ensures the integrity of the code while the workflow is undisturbed.

\section{The different tools chosen}
After evaluating multiple tools for securing the pipeline, the group has given recommendations regarding the various scans. Although the group mainly decided to use integrated tools within GitHub, the decision was primarily based on their suitability for the given task and Erik Hjelmås' recommendation (see the meeting in Appendix \ref{møteErik}). It is important to note that this does not necessarily imply that others should adopt the same tools. Conducting such tests is essential in application development, and selecting tools for carrying out these tests should be based on specific requirements. Therefore, it is necessary to explore the various tools available for different tests and assess whether they meet one's particular needs. The tools selected by the group are examples of what can be used. 

\subsection{Why OWASP ZAP was chosen}
In the course of researching security tools, an evaluation was made of the three tool types: \acrshort{sast},  \acrshort{dast}, and \acrshort{sca}. As neither GitHub nor \acrshort{aws} provides a DAST tool, a third-party tool had to be used. Among these, the \acrshort{owasp} \acrshort{zap} appeared as the most widely used web application scanner globally \cite{owaspzap1}. Also, \acrshort{aws} referenced the \acrshort{dast} tool in an example pipeline using open source tools \cite{inofpipeline}. This tool is convenient due to its user-friendly interface and straightforward setup process, suitable for individuals of all skill levels. Furthermore, it is open-source and available to all users without additional costs. Its integration into \acrshort{aws} \gls{pipeline}s was found to be a simple and uncomplicated process.

\subsection{Why tools integrated into GitHub were chosen}
Initially, the thesis aimed to investigate third-party tools, like Snyk and Mend, which could be integrated into GitHub and run scans on the code pushed to the repository. Then, however, it was decided to look into integrated tools in GitHub. The goal was to explore the potential benefits of using pre-integrated tools to create a secure \gls{pipeline} and give priority to overall functionality rather than choosing individual tools.

\subsection{The group's experience with CodeQL}
As mentioned in section \ref{codeql1}, the group used CodeQL as their \acrshort{sast} tool. CodeQL is set as default when enabling code scanning, simplifying the setup process by requiring fewer steps than other code scanning tools. Furthermore, using the tool in this project was uncomplicated since the only customisation was the trigger, explained in Section \ref{Managing security in GitHub}. However, the group did not explore any further customisation and is therefore not familiar with the level of complication this brings.
\\~\\
CodeQL, in collaboration with Dependabot, found all 101 vulnerabilities reported in the vulnerable-by-design web application \acrshort{owasp} Juice Shop. However, Juice Shop is a widely used repository, which might be why CodeQL and Dependabot managed to find all the vulnerabilities. The group could have considered testing additional repositories for known vulnerabilities to conduct a more comprehensive evaluation of the tool. However, due to the scope limitations, the group decided not to conduct further research on this topic.
\\~\\
Overall, the group found the tool intuitive and appreciated its practicality in providing detailed explanations and concrete examples of patches when detecting errors. 

\subsection{The group's experience with Dependabot}
The group utilized Dependabot as an \acrshort{sca} tool, which was relatively straightforward to configure. Similarly to both \acrlong{dast} and \acrlong{sast}, the group conducted an \acrshort{sca} scan on the Juice Shop repository.
\\~\\
Dependabot excels in providing comprehensive explanations of vulnerabilities and suggesting recommendations for resolving them. For instance, if an outdated dependency version is detected, Dependabot advises on the appropriate version to update to. Furthermore, in cases where an update is unavailable, Dependabot suggests that an alternative dependency should be utilized. However, it does not offer specific alternative dependencies for replacement. Nonetheless, the group found this feature beneficial as it provides guidance for maintaining security. 

\subsection{The group's experience with Secret Scanning}
The group also looked into Secret Scanning. While Secret Scanning does not fall strictly under the category of application security testing, the group recognized its value in scanning code for sensitive information such as tokens and \acrshort{api} keys.
\\~\\
Considering that \acrshort{sast} tools primarily focus on scanning code for vulnerabilities that attackers can exploit, the group found it valuable to utilize an additional scanner that specifically checks for secrets.
\\~\\
Similar to Dependabot, configuring Secret Scanning was also a simple process, requiring only a few steps to enable. Due to the ease of use and the value it brings, explained in Section \ref{Secret scanning}, the group considered the tool as highly beneficial.

\subsection{The group's experience with OWASP ZAP}
The group selected \acrshort{owasp} \acrshort{zap} as their \acrshort{dast} tool and found it straightforward to set up. However, instead of customizing their scans, the group relied on pre-made functions due to the lack of extensive documentation on scan customization. Creating customized scans would have been complex and time-consuming, leading to the decision to utilize the pre-existing functions.
\\~\\
Once \acrshort{owasp} \acrshort{zap} was successfully installed and configured with basic settings, the group faced challenges in comprehending the output results and extracting the output from the docker container.

\section{Automation}
Automation is using \gls{iac} to perform tasks instead of doing them manually \cite{automationredhat}. Implementing automation in the organization's systems can eliminate the need for many developers to manage all the different infrastructure elements the organization may have. Furthermore, there are numerous advantages to replacing manual work with automation. It can reduce costs by replacing the manual work of IT professionals with automated processes. Automation can also speed up development by automating repetitive tasks like testing, building, and deploying code, allowing developers to work on more complex jobs \cite{automation}. In addition, it can improve security by reducing the risk of human errors or by automating security testing that can help speed up the detection, verification, and escalation of security issues without requiring manual involvement. The group's decision to use Terraform and construct a pipeline with \gls{iac} is mostly based on the abovementioned benefits. Furthermore, the group attempted to automate as many processes as possible to remove the need for manual involvement during deployment and testing. 
\\~\\
Despite the numerous benefits of automating the development processes, the implementation can be time-consuming and require a complex setup. However, once completed, the setup would be significantly less complicated and require less time. One significant benefit of automating the development process is the ability to reuse the automated configuration across several projects \cite{reusepipeline}. Automating the process once may be reused in various settings, allowing numerous projects to benefit from the same improved \gls{pipeline}. 
\\~\\
A goal should be to create an \gls{idempotent} process that assures the pipeline's output is consistent every time it is built \cite{idempotent}. Adopting idempotence as a practice in DevOps is an approach during application development that ensures a high-quality experience for both users and software teams. Idempotence eliminates the requirement for post-deployment cleanup, lowering the likelihood of errors. 

\section{The Use of Security Framework}
This section concerns incorporating the two frameworks mentioned in Section \ref{frameworks} into the thesis, even though it was not initially considered. By examining the principles and requirements of these frameworks, valuable perspectives and insights can be discovered. In addition, by comparing the work with frameworks, one can identify areas where improvement is possible and any missing components.

\subsection{SLSA}
The group tried to implement the \acrshort{slsa} framework in the practical part of this project to enhance the integrity of the code we developed. This led the group to attempt to work through the different levels and requirements mentioned in the section \ref{Supply-chainLevelsforSoftwareArtifacts}.

\textbf{\acrshort{slsa} Level 1, requirement 1:}
\textit{\say{Software producer follows
a consistent build process
so that others can form
expectations about what a
“correct” build looks like.}} \cite{SLSAlevels}
\\~\\
This requirement was achieved by creating instructions in the README.md file describing how to build and execute the code in the GitHub project\footnote{Available at: \url{https://github.com/DCSG2900-Bachelor-thesis/CodePipline/blob/main/README.md}}. 
\\~\\
\textbf{\acrshort{slsa} Level 1, requirement 2:}
\textit{\say{Build platform automatically generates provenance describing how the artifact was built, including: what entity built the package, what build process they used, and what the top-level input to the build were.}} \cite{SLSAlevels}
\\~\\
This requirement was not achieved because the group encountered difficulties when trying to find a suitable solution to generate \gls{provenance}. A sentiment shared by Kelsey Hightower in his talk at Strange Loop 2022 \cite{The-Secure-Software-Supply-Chain} highlighting the primary issue with the \acrshort{slsa} framework's high complexity. This complexity was also encountered by the group while searching for a solution.
\\~\\
\textbf{\acrshort{slsa} Level 1, requirement 3:}
\textit{\say{Software producer distributes provenance to consumers, preferably using a convention determined by the package ecosystem.}} \cite{SLSAlevels}
\\~\\
Due to the identical issue encountered in Requirement 2, the group was unable to accomplish Requirement 3.
\\~\\
To conclude, the group could not find a suitable solution for generating \gls{provenance}. As a result, they were unable to advance to the next level, since all the requirements from the previous level must be fulfilled.

\subsection{SSDF}
While numerous models, such as waterfall, spiral, and agile, exist for the \acrlong{sdlc}, only a few prioritize security, therefore, is it necessary to integrate security into the \acrshort{sdlc} models. Utilizing a security-focused framework, such as the \acrshort{ssdf}, as a reference can serve as a starting point for enhancing the security of the \acrshort{sdlc}. 
\\~\\
In \acrshort{ssdf} documentation, elaborated in section \ref{ssdf}, includes various practices covering different security aspects that can be implemented into the development process. While the \acrshort{ssdf} is a suitable framework for secure software development, the group did not explicitly consider it while developing and designing the \gls{pipeline} in the thesis. However, the group believes that the practical work in the thesis aligns with many of the practices and principles of the \acrshort{ssdf}. The group reviewed some examples of the practices they believed to be suitable and their implementation methods. The group has decided to review the practices category-wise, with each practice being assigned to a specific category. 
\\~\\
\textbf{Prepare the Organization (PO):}
Implement Supporting Toolchains (PO.3): \textit{\say{Use automation to reduce human effort and improve the accuracy, reproducibility, usability, and comprehensiveness of security practices throughout the SDLC, as well as provide a way to document and demonstrate the use of these practices. Toolchains and tools may be used at different levels of the organization, such as organization-wide or project-specific, and may address a particular part of the SDLC, like a build pipeline.}}\cite{ssdf}
\\~\\
This practice recommends selecting the right tools for the toolchain, following the recommended security practices, and utilizing tools to generate artifacts. In order to accomplish this, the group used various security tools to run different security tests, all of which comply with best practices for supply chain security. One of the key security practices the group has implemented is to use code-based configuration by automating the \gls{pipeline} with Terraform. The automation of the \gls{pipeline} not only ensures that the pipeline is efficient and practical but also promotes consistency and reduces the likelihood of errors. In \acrshort{aws}, the implementation of this practice is shown by the fact that each stage of the \gls{pipeline} generates an artifact stored in an S3 bucket. 
\\~\\        
\textbf{Protect Software (PS):}
Protect All Forms of Code from Unauthorized
Access and Tampering (PS.1): \textit{\say{Help prevent unauthorized changes to code, both inadvertent and intentional, which could circumvent or negate the intended security characteristics of the software. For code that is not intended to be publicly accessible, this helps prevent theft of the software and may make it more difficult or time-consuming for attackers to find vulnerabilities in the software.}}\cite{ssdf}
\\~\\
This practice focuses on maintaining the integrity and availability of code, as well as proper storage of all code forms. To keep track of the modifications to the code, the group utilizes GitHub's version control and the artifacts generated in \acrshort{aws} if necessary. The group also uses signed commits to ensure code integrity. To follow the \say{least privilege} principle, the group decided to give each member access based on their assigned tasks. Therefore, the code repository was accessible in read-only to the two members responsible for writing the report. In comparison, the other two members, responsible for practical work, were granted full access. 
\\~\\
\textbf{Produce Well-Secured Software (PW):} Review and/or Analyze Human-Readable
Code to Identify Vulnerabilities and Verify Compliance with Security Requirements (PW.7): \textit{\say{Help identify vulnerabilities so that they can be corrected before the software is released to prevent exploitation. Using automated methods lowers the effort and resources needed to detect vulnerabilities. Human-readable code includes source code, scripts, and any other form of code that an organization deems human-readable.}}\cite{ssdf}
\\~\\
This practice covers code security and provides guidelines for evaluating the source code used or written. The evaluation can be done through manual code review or automated tools. The goal is to identify and correct vulnerabilities before releasing the software to prevent exploitation. The group has incorporated various automated security scans throughout the pipeline, such as \acrshort{sca}, \acrshort{sast}, and \acrshort{dast}. 
\\~\\
\textbf{Respond to Vulnerabilities (RV):}
Identify and Confirm Vulnerabilities on an Ongoing Basis (RV.1): \textit{\say{Help ensure that vulnerabilities are identified more quickly so that they can be remediated more quickly in accordance with risk, reducing the window of opportunity for attackers.}}\cite{ssdf}
\\~\\
This practice includes doing frequent vulnerability scans, threat assessments, and penetration testing, as well as monitoring and analyzing system logs and network traffic for indications of possible security problems. Various automated security scans, such as \acrshort{sca}, \acrshort{sast}, and \acrshort{dast}, have been implemented throughout the \gls{pipeline}. These scans help detect potential vulnerabilities and security concerns in the code. Apart from the scans, the group also reflected on the option of application monitoring. This involves conducting assessments post-deployment to ensure the application remains secure and no new vulnerabilities have emerged.

\subsection{Usefulness of the framework}

Both the \acrshort{slsa} and \acrshort{ssdf} frameworks may give helpful recommendations to developers trying to improve the security of their products. However, while \acrshort{slsa} has the potential to encourage good code heritage and increase security in more significant open-source projects with larger user bases, its complexity may not be justified in smaller projects. \acrshort{ssdf}, on the other hand, provides a flexible way to apply security measures, making it a helpful guide for deciding which measures to deploy. Furthermore, the \acrshort{ssdf} may be used by developers to assess the security level of their project without needing an extra step in the development process. Ultimately, the project's unique demands and scope will determine the decision on which framework to use, or whether to use a framework at all.

\section{Revising the thesis angle}
When writing the project plan, the group initially planned on writing a thesis based on testing security tools and comparing the results. The main focus would be on the tools and testing as many as possible. The group found it challenging to create a unique thesis due to previous theses with similar topics. 
\\~\\
After a meeting with the group's professor Erik Hjelmås  (\ref{møteErik}), discussing this issue, the group found an alternative approach for the thesis based on a report from Usenix \cite{usenixreport} shown to the group. The new approach involved incorporating more practical work with \gls{iac} and focusing on utilizing tools already integrated into GitHub and \acrshort{aws}, and implementing best practice security measures. This approach of the thesis seemed to be more aimed at professional life and what kind of research was needed today.

\section{Expectations compared to reality}
\subsection{Practical work}
As mentioned in Section \ref{section: Knowledge that had to be acquired}, the group needed to learn more about \gls{iac}, especially Terraform. During this process, the group faced more issues than anticipated while building the \gls{pipeline} with Terraform, \acrshort{aws} and GitHub. The group had some experience with Microsoft Azure from previous courses and thought that \acrshort{aws} was more similar to Azure than it is. Additionally, \acrshort{aws} provides a wide range of services that targets the same or similar issues. The process of selecting these services and integrating them has posed a more significant challenge than initially anticipated. 

\subsection{Research}
The group struggled to find proper academic research for their thesis. Therefore, the majority of their resources came from websites or blog posts. To establish the credibility of these sources, the group either confirmed the author's or company's credibility or cross-referenced the information with additional sources to confirm its accuracy. 


\section{Critique of the thesis}

\subsection{Not using frameworks from the beginning}
The group did not look deeper into following a framework as the scope contained no specifications. However, when the group discovered different frameworks, the group decided that it was too late to integrate them into the work. Doing so would have required a significant amount of work, and since the group was too far into the thesis, it was decided not to look into it—many requirements for the different frameworks needed to be achieved before the thesis was started. Furthermore, many tasks must be completed if the group were to integrate the framework later in the thesis, which would have added more complexity. 

\subsection{Defining the scope}
At first, the group needed help understanding the stakeholder's scope and requirements, which caused a delay in starting the thesis. The group was given some flexibility in defining the scope but needed help to learn the topic entirely. As a result, they spent extra time and effort researching and consulting with professors to gain a better understanding. This helped them define the scope more accurately and align their work accordingly.


