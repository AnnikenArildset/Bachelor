\chapter{Discussion}
\section{Introduction}
This chapter consists of a justification of the group's choices during the construction of the pipeline, changes that were made during the project, and limitations that were encountered.  


\section{The groups use of the SDLC} %Bytte navn perhaps? 
Working on the thesis, a product consisting of a report and code was created. Though this is not software, the process has some similarities to software development. Therefore, the group decided to consider using the \acrshort{sdlc} in their work.
\\~\\
During the planning phase, the group focused on gathering requirements for the thesis and developing a project plan. Initially, the group created a plan for using third-party tools for the scans, and set an estimate of the costs. However, the plan of using third-party tools was later discarded, with the exception of the \acrshort{dast} tool. The project plan included a Gantt chart that outlined the expected timeframe for tasks. Additionally, the group utilized Jira\footnote{Available at \url{https://www.atlassian.com/software/jira}} to create a Kanban board for task assignments. To prepare for pipeline implementation, the group created several models that detailed its structure, including security scans. The group also became familiar with the software and tools that would be utilized for the pipeline.
\\~\\
In the implementation phase, the group began the development of the pipeline while simultaneously working on the thesis. The coding process for the pipeline involved repeatedly implementing code and testing its functionality. For instance, the group implemented the code for \acrshort{aws} CodeBuild and tested whether the service was created in accordance with the code. As a result, the implementation and testing phases were carried out iteratively.
\\~\\
The end result was ultimately merged into a GitHub repository and documented in the thesis. As the product is not an application and is not intended for direct use in the future, but rather as a demonstration, no plans have been made for its maintenance. 

\section{Chosen branch protection rules}
When deciding which branch protection rules to implement, it is important that securing the branch does not drastically affect the workflow. There are several possible protection rules to enable, described in section \ref{branchprotection}. Though, enabling all of the rules at once may do more harm than good, as every pull request and commit must go through several steps which will make the workflow less efficient. Therefore, selecting only the most useful rules will benefit the development. 
\\~\\
Enabling "Require a pull request before merging", is based on the "Four Eyes Principle" which is described more in detail in section \ref{Security of the pipeline}. By enabling this rule, the risk of unwanted code being merged into the basecode is lower, without disrupting the work of all developers since only two need to look through the code. \cite{foureyes} 
\\~\\
Another implemented branch rule is requiring signed commits. Signed commits require some pre-configurations, but once that is done, all the developers have to do is enter a passphrase, and the commit is verified. Implementing required signed commits ensures the integrity of the code while the workflow is undisturbed.

\section{The different tools chosen}
After evaluating multiple tools utilized for securing the pipeline, the group has given perspectives and recommendations regarding the various scans. Although the group decided to use integrated tools within GitHub for performing \acrshort{sast} and \acrshort{sca} testing, as well as a third-party tool for \acrshort{dast} testing, the decision was primarily based on their suitability for the given task and Erik Hjelmås' recommendation (see the meeting in Appendix \ref{møteErik}). It is crucial to note that this does not necessarily imply that others should adopt the same tools. While conducting such tests is crucial in application development, the selection of tools for carrying out these tests should be based on specific requirements. Therefore, it is essential to explore the various tools available for different tests and assess whether they meet one's specific needs. The tools selected by the group are examples of what can be used. 

\subsection{Why OWASP ZAP was chosen}
When the group started the research on different \acrshort{sast}, \acrshort{dast} and \acrshort{sca} tools, one of the \acrshort{dast} tools that were rapidly mentioned was \acrshort{owasp} \acrshort{zap} - which claims to be the world's most widely used web application scanner. 
The group then discovered during research that there were some advantages to \acrshort{owasp} \acrshort{zap}, one being easy to use and set up. Since ZAP is intended for use by people of diverse skill levels, the group thought it would be a beneficial tool because of minimal expertise with penetration testing. Another advantage is that \acrshort{owasp} \acrshort{zap} is a free open-source tool, which makes it possible to try all functionality the software has to offer without any costs. OWASP ZAP is used when building \gls{pipeline}s in \acrshort{aws} and was easy to implement in \acrshort{aws}.

\subsection{Why tools integrated in GitHub was chosen}
Initially, the thesis aimed to investigate third-party tools, like Snyk and Mend, which could be integrated into GitHub and run scans on the code pushed to the repository. However, it was decided to look into the in-built tools within GitHub and AWS that perform \acrshort{sast}, \acrshort{dast}, and \acrshort{sca} tests. This was to investigate the possibility of utilizing pre-integrated tools for establishing a secure pipeline and prioritizing the end-to-end functionality of the pipeline, instead of determining the best tools.

\subsection{The group's experience with CodeQL}
The group used CodeQL as their \acrshort{sast} tool, which as mentioned above is an integrated tool in GitHub and is set as default when enabling Code Scanning. The group found it uncomplicated to set up and customize it for personal use. For example, the tool was configured to be triggered only when a push to the main branch was done, which was easily achievable. 
\\~\\
The group created a separate repository where OWASP Juice-Shop code was forked and then enabled CodeQL for this repository. After enabling CodeQL, the group enabled notification, so every time it found a vulnerability all the group members would be notified by email. However, due to the many vulnerabilities within the code, the group quickly decided not to continue with the notifications since the group got a significant amount of emails. Nonetheless, if the group were to decide only to be notified every time the tool found vulnerabilities that were considered high or critical, the number of emails would have been significantly lower. Even though the group decided to turn off the notifications, it continued to find vulnerabilities. Lastly, CodeQL in collaboration with Dependabot managed to find all 101 vulnerabilities\footnote{Available at: https://owasp.org/www-project-juice-shop/} reported in the vulnerable-by-design web application OWASP Juice Shop. 
\\~\\
However, JuiceShop is a widely used repository, which might be a reason why CodeQL together with Dependabot managed to find all the vulnerabilities. If the group were to evaluate the tool more thoroughly, the group would have run other repositories with code containing vulnerabilities or personal code with vulnerabilities only the group knew about. That would probably have made the result even more accurate as it would have given CodeQL a range of different repositories with different vulnerabilities.
\\~\\
However, the group found the tool quite intuitive and thought it was practical that when it found an error, it explained thoroughly what the warning was and even gave an example of how to patch it. The group did not look into the example and saw if it could be considered a good patch, but thought it was good to at least receive a possible fix. It also refers to CWE vulnerabilities. 
\\~\\
To sum up, the group thought it worked well and was easy to set up, but since the group did not test it with other repositories than JuiceShop it is hard to estimate if it can be considered accurate. However, this is out of the group's scope and was therefore not investigated further. 



\subsection{The group's experience with OWASP ZAP}

The group opted for OWASP ZAP as \acrshort{dast} tool and found it quite easy to set up. However, rather than customizing their own scans, the group utilized pre-made functions due to limited documentation on customizing their own scans and time constraints. Creating own scans would quickly be too complicated and therefore take too much time. 
\\~\\
When OWASP ZAP, finally was up and running with basic configurations, the group encountered difficulties with understanding the output of the results and extracting the output from the docker container. 
\\~\\
In contrast to the SAST tool, the group also used JuiceShop code for the  DAST scan, which seem to be quite effective. 
%I will add more here once we have looked at the dast scan. 



\subsection{The group's experience with Dependabot}
Dependabot was used as \acrshort{sca} tool, and was quite easy to set up. When compared to both \acrshort{dast} and \acrshort{sast}, the group ran JuiceShop code through Dependabot, which as mentioned above, managed together with \acrshort{sast} to find all the vulnerabilities within the code. 
Compared with CodeQL, Dependabot provides a detailed explanation of the error and suggests recommendations for fixing it. For instance, if an outdated version of a dependency has been detected, Dependabot recommends which version to update it to and if there is not an update out yet, Dependabot suggests that another dependency should be used. However, it does not come with any alternative dependencies. The group found this feature useful, as it provides suggestions for maintaining security. However, it is important to note that the group did not investigate the suggested fixes, as it was beyond the project´s scope.
\\~\\
In summary, the group found Dependabot to be a valuable due to its ease to set up. However, the group also identified some areas where it could be improved, such as the lack of examples fort alternative dependencies that could be used in case a vulnerability is detected in the current dependency. 


\subsection{The group's experience with Secret Scanning}
Finally, the group explored Secret Scanning, which as mentioned is also an integrated tool in GitHub. Although Secret Scanning is not necessarily an application security testing, the group found it valuable as it scans the code for secrets such as tokens and API keys, which could allow access to sensitive information. If such secrets were exposed, they can be exploited by an attacker to steal data for instance. 
\\~\\
Since \acrshort{sast} tools can be used to scan the code for vulnerabilities that an attacker can exploit, the group considered it good practice to use another scanner that checks for secrets as well. 
\\~\\
Compared to different application security tests, Secret Scanning was also quite easy to set up. Thus, the group decided it would be beneficial to use it. It enhances security and can also be configured to trigger when specific events occur, making it a profitable tool to use. 
\\~\\
Secret Scanning also gives a detailed explanation of the errors and how to solve them, which seems to be a usual thing that GitHub tools do. 

\section{Automation}
Automation is the use of \gls{infrastructure as code} to perform tasks, instead of doing them manually. Implementing automation in the organization's systems can eliminate the need for many developers to manage all the different infrastructure elements the organization may have. Furthermore, there are numerous advantages to replacing manual work with automation. It can reduce costs, by replacing the manual work of IT professionals with automated processes or by reducing the need for physical hardware, such as servers, by moving to a cloud-based solution. Automation can also speed up development by automating repetitive tasks, such as testing, building, and deploying code, allowing developers to work on more complex jobs. In addition, it can improve security by reducing the risk of human errors, or by automating security protocols that can help to speed up the detection, verification, and escalation of security issues without the need for manual involvement. The group's decision to use Terraform and construct a pipeline with \gls{infrastructure as code} is mostly based on the benefits mentioned above. Furthermore, the group attempted to automate as many processes as possible in order to remove the need for manual involvement during deployment and testing. \cite{automation} \cite{automationredhat}
\\~\\
Despite the numerous benefits of automating the development processes, the implementation can be time-consuming and require a complex setup. However, once completed, the setup would be significantly less complicated and require less time. A goal should be to create an \gls{idempotent} process that assures the pipeline's output is consistent every time it is built. Adopting idempotence as a practice in DevOps is an approach during application development that ensures a high-quality experience for both users and software teams. Idempotence eliminates the requirement for post-deployment cleanup, lowering the likelihood of errors. \cite{idempotent}

\section{The Use of Framework}
\subsection{SLSA}

Consistency is crucial when building your code, as previously mentioned in \acrshort{slsa} level 1. Therefore, it is mandatory to have a method of instructing others on how to build your code so that the outcome is consistent every time. The most common way of achieving this, and the method our group has chosen, is through a README.md file in the code repository. 

The second point the group had to deal with was to have a build platform that could automatically create \gls{provenance}, creating a document that provides information about what was built, the method used to build it, and the exact time it was built. This is where the problem with the \acrshort{slsa} framework and \acrshort{iac} code arises. For many other programming languages, you have to build them in advance, but with \acrshort{iac} languages, you do not build them in advance before running them.

Failing to achieve point two makes it challenging to proceed to point three in \acrshort{slsa} level 1. Without generating a provision, we have no content to share publicly with other users. It's worth noting that this obstacle also applies to levels 2 and 3, as completing level 1 is a prerequisite for advancing to these levels.

\subsection{SSDF}
While numerous models exist for the \acrlong{sdlc}, only a limited number of them prioritize security. As a result, it is necessary to integrate security into the \acrshort{sdlc} models. Utilizing a security-focused framework, such as the \acrshort{ssdf}, as a reference can serve as a starting point for enhancing the security of the \acrshort{sdlc}. 
\\~\\
In \acrshort{ssdf} documentation, which was elaborated in section \ref{ssdf}, includes various of  practices covering a wide range of security aspects that can be implemented into the development process. While the SSDF is a suitable framework for secure software development, the group did not explicitly have it in mind while developing and designing the pipeline in the thesis. However, the group believes that the practical work in the thesis aligns with many of the practices and principles of the SSDF, even though it wasn't explicitly followed. The group reviewed some examples of the practices to which they believed to be suitable and their methods of implementation. The group has decided to review the practices in a category-wise manner, with each practice being assigned to a specific category. 
\\~\\
\textbf{Under Prepare the Organization (PO):}

Implement Supporting Toolchains (PO.3): \textit{\say{Use automation to reduce human effort and improve the accuracy, reproducibility, usability, and comprehensiveness of security practices throughout the SDLC, as well as provide a way to document and demonstrate the use of these practices. Toolchains and tools may be used at different levels of the organization, such as organization-wide or project-specific, and may address a particular part of the SDLC, like a build pipeline.}}\cite{ssdf}
\\~\\
This practice recommends selecting the right tools for the toolchain, following the recommended security practices, and utilizing tools to generate artifacts. In order to accomplish this, the group used a variety of security tools to run different security tests, all of which comply with best practices for supply chain security. One of the key security practices the group has implemented is to use code-based configuration by automating the pipeline with Terraform. The automation of the pipeline not only ensures that the pipeline is efficient and effective, but also promotes consistency and reduces the likelihood of errors. In \acrshort{aws}, the implementation of this practice is shown by the fact that each stage of the pipeline generates an artifact stored in an S3 bucket. This ensures that the artifacts created are safe, secure, and available for future use.
\\~\\        
\textbf{Protect Software (PS):}
Protect All Forms of Code from Unauthorized
Access and Tampering (PS.1): \textit{\say{Help prevent unauthorized changes to code, both inadvertent and intentional, which could circumvent or negate the intended security characteristics of the software. For code that is not intended to be publicly accessible, this helps prevent theft of the software and may make it more difficult or time-consuming for attackers to find vulnerabilities in the software.}}\cite{ssdf}
\\~\\
The focus of this practice is on maintaining the integrity and availability of code, as well as proper storage of all code forms. To keep track of the modifications to the code, the group utilizes GitHub's version control and the artifacts generated in \acrshort{aws} if necessary. Signed commits are also used by the group to ensure code integrity. To follow the \say{least privilege} principle, the group decided to give each member access based on the specific tasks assigned to them. The code repository was accessible in read-only to only the two members who were responsible for writing the report, while the other two members, who were responsible for practical work, were granted full access. 

\textbf{Produce Well-Secured Software (PW):} Review and/or Analyze Human-Readable
Code to Identify Vulnerabilities and Verify Compliance with Security Requirements (PW.7): \textit{\say{Help identify vulnerabilities so that they can be corrected before the software is released to prevent exploitation. Using automated methods lowers the effort and resources needed to detect vulnerabilities. Human-readable code includes source code, scripts, and any other form of code that an organization deems human-readable.}}\cite{ssdf}
\\~\\
This practice covers code security and provides guidelines on how to evaluate the source code that is being used. This can be done through manual code review or through the use of automated tools.  The goal is to identify and correct vulnerabilities before the software is released, to prevent exploitation. The group has incorporated various automated security scans throughout the pipeline, such as SCA, SAST, and DAST. 


\textbf{Respond to Vulnerabilities (RV):}
Identify and Confirm Vulnerabilities on an Ongoing Basis (RV.1): \textit{\say{Help ensure that vulnerabilities are identified more quickly so that they can be remediated more quickly in accordance with risk, reducing the window of opportunity for attackers.}}\cite{ssdf}
\\~\\This practice includes doing frequent vulnerability scans, threat assessments, and penetration testing, as well as monitoring and analyzing system logs and network traffic for indications of possible security problems. Various automated security scans, such as SCA, SAST, and DAST tests, have been implemented throughout the pipeline. These scans help in the detection of potential vulnerabilities and security concerns in the code. In addition to these scans, the group discussed the possibility of application monitoring. This means performing tests after deployment to confirm that the application is still secure and no new vulnerabilities are created. 

\subsection{Usefulness of the framework}
\textbf{SLSA}
While the framework has some usefulness in terms of promoting good code heritage, its major issue is the significant amount of work needed to set it up and its high level of complexity that makes it difficult to work with. For a small project like ours, this complexity is not justified. However, the group can appreciate the value of this framework for larger open-source projects with broader user bases. Nevertheless, for the current size and scope, the complexity remains the biggest problem with this framework.

\textbf{SSDF}



\section{Revising the Thesis Angle}
When writing the project plan, the group initially planned to write the thesis where various security testing tools would be tested and analyzed, and later on, demonstrate how the different tools could be used together. The main focus would be on the tools, and try to test as many tools as possible. The group found it difficult to create a unique thesis due to previous theses with similar topics. 
\\
After a meeting with the group's professor Erik Hjelmås  (\ref{møteErik}) discussing this issue, the group found an alternative approach for the thesis, based on the discussion with Erik and a report from Usenix \cite{usenixreport} shown to the group. The new approach involved incorporating more practical work with infrastructure-as-code and focusing on utilizing tools already integrated into GitHub and AWS and implementing best practice security measures. This approach of the thesis seemed to be more aimed at professional life, and what kind of research was needed today.  


\section{Expectations Compared to Reality}
\subsection{Practical work}
As mentioned in the section \ref{section: Knowledge that had to be acquired}, the group had little to no knowledge about \gls{infrastructure as code}, especially not Terraform. The group faced more issues than anticipated while building the pipeline with Terraform and \acrshort{aws} during this process.
The group had some experience with Microsoft Azure from previous courses and perhaps thought that \acrshort{aws} was a bit more similar to Azure than it actually is. Additionally, \acrshort{aws} provides a wide range of services that targets the same or similar issues. The process of selecting these services and integrating them with each other has posed a greater challenge than initially anticipated. 


\subsection{Research}
The group struggled to find proper academic research for their thesis, therefore the majority of their resources came from websites or blog posts. To establish the credibility of these sources, the group either confirmed the author's or company's credibility or cross-referenced the information with additional sources to confirm its accuracy. These precautions were taken to assure the accuracy and validity of the data in the thesis.


\section{Critique of the Thesis}

\subsection{Not using Framework from the beginning}
The group did not look deeper into following a framework as the scope did not contain any specifications regarding it. However, when the group discovered different frameworks, the group decided that it was too late to integrate them into the work. Doing so would have required a significant amount of work, and since the group was too far into the thesis, it was decided to not look into it. A lot of the requirements for the different frameworks was needed to be achieved before the thesis was started. There were also many tasks that needed to be completed if the group were to integrate the framework later in the thesis, which would have added more complexity to the thesis. 

\subsection{Defining the scope}
Initially, the group struggled with understanding the scope given by the stakeholder and the requirements that followed. The group was given a lot of leeway and was told to interpret the scope however suited, which due to lack of knowledge about the topic made it difficult to establish what the scope was really asking. As a result, the group had to invest more time and work in the beginning to ensure that the thesis became the best possible. As mentioned above the group for example had a talk with Erik Hjelmås regarding the scope and how to optimize it. 

